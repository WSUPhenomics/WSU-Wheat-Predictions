{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Segmentation using PyTorch\n",
    "\n",
    "In this notebook, we will perform image segmentation on images stored in the `data/cam1` directory using a pre-trained U-Net model.\n",
    "\n",
    "## Step 1: Setup and Imports\n",
    "\n",
    "Before we import the libraries, we need to install the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install torch torchvision numpy matplotlib opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move onto import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.models.segmentation import fcn_resnet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## Step 2: Define the Dataset\n",
    "\n",
    "We need to define a custom dataset class to handle the images in the `data/cam1` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(image_folder) if f.endswith('.png')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_folder, self.image_files[idx])\n",
    "        image = Image.open(img_name)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = ImageDataset(image_folder='../data/cam1', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load a Pre-trained Model\n",
    "\n",
    "We will use a pre-trained Fully Convolutional Network (FCN) for segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the pre-trained FCN model\n",
    "model = fcn_resnet50(pretrained=True)\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Perform Segmentation and Visualize Results\n",
    "\n",
    "We will iterate over the images, perform segmentation, and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Load and display the image\n",
    "img = mpimg.imread('../data/plot_tool.png')\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = '../data/'\n",
    "image_files = [path + 'cam1_plot_coverage.png', path + 'cam2_plot_coverage.png', path + 'cam3_plot_coverage.png',\n",
    "               path + 'cam4_plot_coverage.png', path + 'cam5_plot_coverage.png', path + 'cam6_plot_coverage.png',\n",
    "               path + 'cam7_plot_coverage.png', path + 'cam8_plot_coverage.png']\n",
    "\n",
    "# Assuming 'image_files' is the list of image file names\n",
    "for image_file in image_files:\n",
    "    # Load the image\n",
    "    img = plt.imread(image_file)\n",
    "\n",
    "    # Display the image with the title as the image file name\n",
    "    plt.imshow(img)\n",
    "    image_file = image_file.replace('../data/', '')\n",
    "    plt.title(image_file)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((928, 1280)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(image_folder) if os.path.isfile(os.path.join(image_folder, f))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_folder, self.image_files[idx])\n",
    "        image = Image.open(img_name).convert('RGB')  # Convert RGBA to RGB\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "def split_image(image):\n",
    "    left_image = image[:, :, :640]\n",
    "    right_image = image[:, :, 640:]\n",
    "    return left_image, right_image\n",
    "\n",
    "# Load the dataset\n",
    "dataset_folder = '../data/cam1'\n",
    "output_folder = '../data/cam1_split'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "dataset = ImageDataset(image_folder=dataset_folder, transform=transform)\n",
    "\n",
    "# Iterate through the dataset, split images, and display them\n",
    "for idx in range(len(dataset)):\n",
    "    image = dataset[idx]  # Get image as a tensor\n",
    "    left_image, right_image = split_image(image)\n",
    "\n",
    "    # Convert tensors back to PIL Images\n",
    "    left_image_pil = transforms.ToPILImage()(left_image)\n",
    "    right_image_pil = transforms.ToPILImage()(right_image)\n",
    "\n",
    "    # Save the split images\n",
    "    base_filename = os.path.splitext(dataset.image_files[idx])[0]\n",
    "    left_image_pil.save(os.path.join(output_folder, f\"{base_filename}_left.png\"))\n",
    "    right_image_pil.save(os.path.join(output_folder, f\"{base_filename}_right.png\"))\n",
    "\n",
    "    # Display the images\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(left_image_pil)\n",
    "    plt.title(f\"{base_filename}_left.png\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(right_image_pil)\n",
    "    plt.title(f\"{base_filename}_right.png\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
